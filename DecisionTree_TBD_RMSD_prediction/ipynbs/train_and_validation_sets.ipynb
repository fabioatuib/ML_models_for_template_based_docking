{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "#data\n",
    "import pandas as pd\n",
    "\n",
    "# chemistry\n",
    "import rdkit\n",
    "from rdkit import RDLogger\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem import Lipinski\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Machine learning\n",
    "import sklearn\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here I am defining a validation set and a training set:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# get the full data set\n",
    "df = pd.read_csv('../../../Analysis_of_Docking/data/rmsd_values_featurized_w_sasa_without_bad_pairs.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I am going to group together pairs that have each other in common:\n",
    "\n",
    "to guarantee that the training data is different from the testing and validation data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# create a list of all the combinations of pairs of compounds that exist in the data set\n",
    "# the pair (\"A\"_template, \"B\"_docked) is the same as (\"B\"_docked, \"A\"_template)\n",
    "list_of_sets = []\n",
    "# it is a list of lists of indexes\n",
    "list_of_indexes = []\n",
    "for index, template, docked in df[['template', 'docked']].itertuples():\n",
    "    if {template, docked} not in list_of_sets:\n",
    "        list_of_sets += [{template, docked}]\n",
    "        list_of_indexes += [[index]]\n",
    "    else:\n",
    "        list_of_indexes[list_of_sets.index({template, docked})] += [index]\n",
    "\n",
    "# group together all sets until all the sets are disjoint in relation to each other\n",
    "list_of_sets_copy = list_of_sets.copy()\n",
    "list_of_lists = [[]]\n",
    "list_of_lists_indexes = [[]]\n",
    "while_loop_controller_1 = True\n",
    "while while_loop_controller_1:\n",
    "    set1 = list_of_sets_copy[0]\n",
    "    tempset = set()\n",
    "    tempset.update(set1)\n",
    "    while_loop_controller_2 = True\n",
    "    while while_loop_controller_2:\n",
    "        i = 0\n",
    "        i_s = []\n",
    "        for set2 in list_of_sets_copy:\n",
    "            if not tempset.isdisjoint(set2):\n",
    "                tempset.update(set2)\n",
    "                list_of_lists[-1] += [set2]\n",
    "                list_of_lists_indexes[-1] += list_of_indexes[list_of_sets.index(set2)]\n",
    "                i_s += [i]\n",
    "            i += 1\n",
    "        if len(i_s) == 0:\n",
    "            while_loop_controller_2 = False\n",
    "        list_of_sets_copy = [list_of_sets_copy[j] for j in range(len(list_of_sets_copy)) if j not in i_s]\n",
    "    if len(list_of_sets_copy) > 0:\n",
    "        list_of_lists += [[]]\n",
    "        list_of_lists_indexes += [[]]\n",
    "    else:\n",
    "        while_loop_controller_1 = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "size_of_groups = {}\n",
    "df['group'] = -1\n",
    "group = 0\n",
    "for list in list_of_lists_indexes:\n",
    "    group += 1\n",
    "    size_of_groups[group] = len(list)\n",
    "    for index in list:\n",
    "        df.at[index, 'group'] = group"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 7399, 2: 4296, 3: 2421, 4: 6, 5: 788, 6: 1053, 7: 726, 8: 1215, 9: 408, 10: 695, 11: 593, 12: 260, 13: 106, 14: 2, 15: 94, 16: 39, 17: 5, 18: 119, 19: 504, 20: 2, 21: 483, 22: 313, 23: 248, 24: 222, 25: 324, 26: 314, 27: 39, 28: 199, 29: 218, 30: 93, 31: 2, 32: 13, 33: 190, 34: 162, 35: 20, 36: 2, 37: 3, 38: 55, 39: 92, 40: 76, 41: 89, 42: 84, 43: 103, 44: 116, 45: 48, 46: 118, 47: 74, 48: 11, 49: 17, 50: 37, 51: 53, 52: 6, 53: 45, 54: 23, 55: 2, 56: 37, 57: 48, 58: 22, 59: 7, 60: 7, 61: 11, 62: 20, 63: 8, 64: 7, 65: 11, 66: 2, 67: 5, 68: 6, 69: 8, 70: 2, 71: 2, 72: 2, 73: 2, 74: 4, 75: 4, 76: 1, 77: 6}\n"
     ]
    }
   ],
   "source": [
    "# the size of the different disjoint groups is variable\n",
    "print(size_of_groups)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "([array([46,  4, 36,  8,  3, 16, 51, 68, 54, 50, 64,  7, 31, 13, 35]),\n  array([27, 72, 48,  2, 66, 61, 60,  6, 18, 24, 55, 53, 75, 25, 26]),\n  array([15, 77, 33, 21, 52, 73, 22, 14, 41, 19, 59, 67, 43, 70,  5]),\n  array([57, 38, 11, 20, 28, 23, 37,  9, 56, 32, 42, 30, 74, 10, 40, 44]),\n  array([29, 58, 71, 17, 47, 63, 45, 62, 65, 49, 34, 76, 69,  1, 39, 12])],\n ([array([ 118,    6,    2, 1215, 2421,   39,   53,    6,   23,   37,    7,\n           726,    2,  106,   20]),\n   array([  39,    2,   11, 4296,    2,   11,    7, 1053,  119,  222,    2,\n            45,    4,  324,  314]),\n   array([ 94,   6, 190, 483,   6,   2, 313,   2,  89, 504,   7,   5, 103,\n            2, 788]),\n   array([ 48,  55, 593,   2, 199, 248,   3, 408,  37,  13,  84,  93,   4,\n          695,  76, 116]),\n   array([ 218,   22,    2,    5,   74,    8,   48,   20,   11,   17,  162,\n             1,    8, 7399,   92,  260])],\n  [4781, 6451, 2594, 2674, 8347],\n  [0.19241759568559585,\n   0.25962892904576007,\n   0.10439892139896165,\n   0.10761862599106532,\n   0.33593592787861715]))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am going to have a validation set of about 20 % the size of the full data set\n",
    "def check_sizes_of_folds(folds):\n",
    "    sizes = []\n",
    "    size_of_components = []\n",
    "    for fold in folds:\n",
    "        sizes += [sum([size_of_groups[g] for g in fold])]\n",
    "        size_of_components += [np.array([size_of_groups[g] for g in fold])]\n",
    "    proportions =  [sizes[i]/sum(sizes) for i in range(len(sizes))]\n",
    "    return size_of_components, sizes, proportions\n",
    "\n",
    "k = 5\n",
    "\n",
    "groups = df['group'].drop_duplicates().values\n",
    "\n",
    "\n",
    "base_size = len(groups)//(k)\n",
    "base_rest = len(groups)%(k)\n",
    "\n",
    "print(base_size, base_rest)\n",
    "\n",
    "random.shuffle(groups)\n",
    "folds = [groups[i*base_size:(i+1)*base_size] for i in range(k)]\n",
    "\n",
    "for i in range(base_rest):\n",
    "    folds[-(i+1)] = np.append(groups[-(i+1)], folds[-(i+1)])\n",
    "\n",
    "folds, check_sizes_of_folds(folds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# the validation fold becomes:\n",
    "# which has a size of 4781 and is about 19% of the total number\n",
    "validation_fold = [46,  4, 36,  8,  3, 16, 51, 68, 54, 50, 64,  7, 31, 13, 35]\n",
    "\n",
    "validation_fold_df = df.loc[df['group'].isin(validation_fold)]\n",
    "\n",
    "# save the validation set\n",
    "validation_fold_df.to_csv('../data/validation_rmsd_values_featurized_w_sasa_without_bad_pairs.csv',\n",
    "                          index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# the set used for choosing hyperparameters and training\n",
    "train_test_df = df.loc[~df['group'].isin(validation_fold)]\n",
    "\n",
    "# save the train_test\n",
    "train_test_df.to_csv('../data/train_test_rmsd_values_featurized_w_sasa_without_bad_pairs.csv',\n",
    "                     index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}